From 33735be7891ec8ea3b97833e08f1d7a1d94d65b6 Mon Sep 17 00:00:00 2001
From: Pradeep Kumar Chitrapu <quic_pradeepc@quicinc.com>
Date: Wed, 10 Aug 2022 23:28:06 -0700
Subject: [PATCH] ath12k: add branch predictors in dp_tx

In datapath, add branch predictors where required in the dp_tx().

Signed-off-by: Pradeep Kumar Chitrapu <quic_pradeepc@quicinc.com>
---
 drivers/net/wireless/ath/ath12k/dp_tx.c | 58 ++++++++++++-------------
 1 file changed, 29 insertions(+), 29 deletions(-)

diff --git a/drivers/net/wireless/ath/ath12k/dp_tx.c b/drivers/net/wireless/ath/ath12k/dp_tx.c
index 8163ca5..fed0c6b 100644
--- a/drivers/net/wireless/ath/ath12k/dp_tx.c
+++ b/drivers/net/wireless/ath/ath12k/dp_tx.c
@@ -97,7 +97,7 @@ static struct ath12k_tx_desc_info *ath12k_dp_tx_assign_buffer(struct ath12k_dp *
 	desc = list_first_entry_or_null(&dp->tx_desc_free_list[ring_id],
 					struct ath12k_tx_desc_info,
 					list);
-	if (!desc) {
+	if (unlikely(!desc)) {
 		spin_unlock_bh(&dp->tx_desc_lock[ring_id]);
 		ath12k_warn(dp->ab, "failed to allocate data Tx buffer\n");
 		return NULL;
@@ -149,11 +149,11 @@ int ath12k_dp_tx(struct ath12k *ar, struct ath12k_vif *arvif,
 	bool tcl_ring_retry;
 	bool msdu_ext_desc = false;
 
-	if (test_bit(ATH12K_FLAG_CRASH_FLUSH, &ar->ab->dev_flags))
+	if (unlikely(test_bit(ATH12K_FLAG_CRASH_FLUSH, &ar->ab->dev_flags)))
 		return -ESHUTDOWN;
 
-	if (!(info->flags & IEEE80211_TX_CTL_HW_80211_ENCAP) &&
-	    !ieee80211_is_data(hdr->frame_control))
+	if (unlikely(!(info->flags & IEEE80211_TX_CTL_HW_80211_ENCAP) &&
+	    !ieee80211_is_data(hdr->frame_control)))
 		return -ENOTSUPP;
 
 	/* Let the default ring selection be based on current processor
@@ -175,7 +175,7 @@ tcl_ring_sel:
 	tx_ring = &dp->tx_ring[ti.ring_id];
 
 	tx_desc = ath12k_dp_tx_assign_buffer(dp, ti.ring_id);
-	if (!tx_desc) {
+	if (unlikely(!tx_desc)) {
 		ab->soc_stats.tx_err.txbuf_na[ti.ring_id]++;
 		if (ring_map == (BIT(DP_TCL_NUM_RING_MAX) - 1))
 			return -ENOSPC;
@@ -207,8 +207,8 @@ tcl_ring_sel:
 	ti.bss_ast_idx = arvif->ast_idx;
 	ti.dscp_tid_tbl_idx = 0;
 
-	if (skb->ip_summed == CHECKSUM_PARTIAL &&
-	    ti.encap_type != HAL_TCL_ENCAP_TYPE_RAW) {
+	if (likely(skb->ip_summed == CHECKSUM_PARTIAL &&
+		   ti.encap_type != HAL_TCL_ENCAP_TYPE_RAW)) {
 		ti.flags0 |= u32_encode_bits(1, HAL_TCL_DATA_CMD_INFO2_IP4_CKSUM_EN) |
 			     u32_encode_bits(1, HAL_TCL_DATA_CMD_INFO2_UDP4_CKSUM_EN) |
 			     u32_encode_bits(1, HAL_TCL_DATA_CMD_INFO2_UDP6_CKSUM_EN) |
@@ -242,15 +242,15 @@ tcl_ring_sel:
 	}
 
 	ti.paddr = dma_map_single(ab->dev, skb->data, skb->len, DMA_TO_DEVICE);
-	if (dma_mapping_error(ab->dev, ti.paddr)) {
+	if (unlikely(dma_mapping_error(ab->dev, ti.paddr))) {
 		atomic_inc(&ab->soc_stats.tx_err.misc_fail);
 		ath12k_warn(ab, "failed to DMA map data Tx buffer\n");
 		ret = -ENOMEM;
 		goto fail_remove_tx_buf;
 	}
 
-	if ((arvif->tx_encap_type == HAL_TCL_ENCAP_TYPE_ETHERNET &&
-	    !(info->flags & IEEE80211_TX_CTL_HW_80211_ENCAP))) {
+	if (unlikely((arvif->tx_encap_type == HAL_TCL_ENCAP_TYPE_ETHERNET &&
+	    !(info->flags & IEEE80211_TX_CTL_HW_80211_ENCAP)))) {
 		msdu_ext_desc = true;
 
 		if (skb->protocol == cpu_to_be16(ETH_P_PAE)) {
@@ -259,7 +259,7 @@ tcl_ring_sel:
 		}
 	}
 
-	if (arvif->tx_encap_type == HAL_TCL_ENCAP_TYPE_RAW) {
+	if (unlikely(arvif->tx_encap_type == HAL_TCL_ENCAP_TYPE_RAW)) {
 		if (skb->protocol == cpu_to_be16(ETH_P_ARP)) {
 			ti.encap_type = HAL_TCL_ENCAP_TYPE_RAW;
 			ti.encrypt_type = HAL_ENCRYPT_TYPE_OPEN;
@@ -285,7 +285,7 @@ tcl_ring_sel:
 	skb_cb->vif = arvif->vif;
 	skb_cb->ar = ar;
 
-	if (msdu_ext_desc) {
+	if (unlikely(msdu_ext_desc)) {
 		skb_ext_desc = dev_alloc_skb(sizeof(struct hal_tx_msdu_ext_desc));
 		if (!skb_ext_desc) {
 			ret = -ENOMEM;
@@ -320,7 +320,7 @@ tcl_ring_sel:
 	ath12k_hal_srng_access_begin(ab, tcl_ring);
 
 	hal_tcl_desc = ath12k_hal_srng_src_get_next_entry(ab, tcl_ring);
-	if (!hal_tcl_desc) {
+	if (unlikely(!hal_tcl_desc)) {
 		/* NOTE: It is highly unlikely we'll be running out of tcl_ring
 		 * desc because the desc is directly enqueued onto hw queue.
 		 */
@@ -334,8 +334,8 @@ tcl_ring_sel:
 		 * checking this ring earlier for each pkt tx.
 		 * Restart ring selection if some rings are not checked yet.
 		 */
-		if (ring_map != (BIT(ab->hw_params->max_tx_ring) - 1) &&
-		    ab->hw_params->tcl_ring_retry) {
+		if (unlikely(ring_map != (BIT(ab->hw_params->max_tx_ring) - 1) &&
+			     ab->hw_params->tcl_ring_retry)) {
 			tcl_ring_retry = true;
 			ring_selector++;
 		}
@@ -379,7 +379,7 @@ static void ath12k_dp_tx_free_txbuf(struct ath12k_base *ab,
 	skb_cb = ATH12K_SKB_CB(msdu);
 
 	dma_unmap_single(ab->dev, skb_cb->paddr, msdu->len, DMA_TO_DEVICE);
-	if (skb_cb->paddr_ext_desc)
+	if (unlikely(skb_cb->paddr_ext_desc))
 		dma_unmap_single(ab->dev, skb_cb->paddr_ext_desc,
 				 sizeof(struct hal_tx_msdu_ext_desc), DMA_TO_DEVICE);
 
@@ -411,7 +411,7 @@ ath12k_dp_tx_htt_tx_complete_buf(struct ath12k_base *ab,
 		wake_up(&ar->dp.tx_empty_waitq);
 
 	dma_unmap_single(ab->dev, skb_cb->paddr, msdu->len, DMA_TO_DEVICE);
-	if (skb_cb->paddr_ext_desc)
+	if (unlikely(skb_cb->paddr_ext_desc))
 		dma_unmap_single(ab->dev, skb_cb->paddr_ext_desc,
 				 sizeof(struct hal_tx_msdu_ext_desc), DMA_TO_DEVICE);
 
@@ -667,7 +667,7 @@ static void ath12k_dp_tx_complete_msdu(struct ath12k *ar,
 	u8 flags = 0;
 
 
-	if (WARN_ON_ONCE(buf_rel_source != HAL_WBM_REL_SRC_MODULE_TQM)) {
+	if (unlikely(WARN_ON_ONCE(buf_rel_source != HAL_WBM_REL_SRC_MODULE_TQM))) {
 		/* Must not happen */
 		return;
 	}
@@ -675,7 +675,7 @@ static void ath12k_dp_tx_complete_msdu(struct ath12k *ar,
 	skb_cb = ATH12K_SKB_CB(msdu);
 
 	dma_unmap_single(ab->dev, skb_cb->paddr, msdu->len, DMA_TO_DEVICE);
-	if (skb_cb->paddr_ext_desc)
+	if (unlikely(skb_cb->paddr_ext_desc))
 		dma_unmap_single(ab->dev, skb_cb->paddr_ext_desc,
 				 sizeof(struct hal_tx_msdu_ext_desc), DMA_TO_DEVICE);
 
@@ -701,12 +701,12 @@ static void ath12k_dp_tx_complete_msdu(struct ath12k *ar,
 
 	ath12k_dp_tx_status_parse(ab, tx_status, &ts);
 
-	if (!rcu_access_pointer(ab->pdevs_active[ar->pdev_idx])) {
+	if (unlikely(!rcu_access_pointer(ab->pdevs_active[ar->pdev_idx]))) {
 		dev_kfree_skb_any(msdu);
 		return;
 	}
 
-	if (!skb_cb->vif) {
+	if (unlikely(!skb_cb->vif)) {
 		dev_kfree_skb_any(msdu);
 		return;
 	}
@@ -731,8 +731,8 @@ static void ath12k_dp_tx_complete_msdu(struct ath12k *ar,
 	    (info->flags & IEEE80211_TX_CTL_NO_ACK))
 		info->flags |= IEEE80211_TX_STAT_NOACK_TRANSMITTED;
 
-	if (ath12k_debugfs_is_extd_tx_stats_enabled(ar) ||
-	    ab->hw_params->single_pdev_only) {
+	if (unlikely(ath12k_debugfs_is_extd_tx_stats_enabled(ar)) ||
+		     ab->hw_params->single_pdev_only) {
 		if (ts.flags & HAL_TX_STATUS_FLAGS_FIRST_MSDU) {
 			if (ar->last_ppdu_id == 0) {
 				ar->last_ppdu_id = ts.ppdu_id;
@@ -757,10 +757,10 @@ static void ath12k_dp_tx_complete_msdu(struct ath12k *ar,
 
 	spin_lock_bh(&ab->base_lock);
 	peer = ath12k_peer_find_by_id(ab, ts.peer_id);
-	if (!peer || !peer->sta) {
-		 ath12k_dbg(ab, ATH12K_DBG_DATA,
-				 "dp_tx: failed to find the peer with peer_id %d\n",
-				 ts.peer_id);
+	if (unlikely(!peer || !peer->sta)) {
+		ath12k_dbg(ab, ATH12K_DBG_DATA,
+			   "dp_tx: failed to find the peer with peer_id %d\n",
+			   ts.peer_id);
 		 spin_unlock_bh(&ab->base_lock);
 		 dev_kfree_skb_any(msdu);
 		 return;
@@ -844,7 +844,7 @@ void ath12k_dp_tx_completion_handler(struct ath12k_base *ab, int ring_id)
 	while (count--) {
 		tx_status = &tx_ring->tx_status[i++];
 
-		if (le32_get_bits(tx_status->info0, HAL_WBM_COMPL_TX_INFO0_CC_DONE)) {
+		if (likely(u32_get_bits(tx_status->info0, HAL_WBM_COMPL_TX_INFO0_CC_DONE))) {
 			/* HW done cookie conversion */
 			desc_va = ((u64)le32_to_cpu(tx_status->buf_va_hi) << 32 |
 				   le32_to_cpu(tx_status->buf_va_lo));
@@ -856,7 +856,7 @@ void ath12k_dp_tx_completion_handler(struct ath12k_base *ab, int ring_id)
 
 			tx_desc = ath12k_dp_get_tx_desc(ab, desc_id);
 		}
-		if (!tx_desc) {
+		if (unlikely(!tx_desc)) {
 			ath12k_warn(ab, "unable to retrieve tx_desc!");
 			continue;
 		}
-- 
2.17.1

