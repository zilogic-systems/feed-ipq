From a17a77c0a87a1f43dcff4893de9030e4d30a5594 Mon Sep 17 00:00:00 2001
From: Karthikeyan Periyasamy <quic_periyasa@quicinc.com>
Date: Mon, 23 Oct 2023 15:16:09 +0530
Subject: [PATCH 6/6] wifi: ath12k: avoid the used list in data path structure

Whenever the REO/WBM/Exception ring receives the packet, the rx descriptor
move from the used list to the free list. Then the rxdma ring gets
replenished where the rx descriptor again moves from the free list to the
used list. At the end, the descriptor retain in the used list, So its an
unnecessary list movement from the used list. Also the descriptor list
operations (add, delete) consume CPU cycles which are per packet in the
Rx data path. To optimize the Rx data path, avoid the descriptor list
movement by removing the used linked list and also remove the per packet
un/lock for the used list operation. Now maintain the used list in the
local variable instead of DP structure. Now there is no need of spin lock
protection, since the local variable is accessed nowhere unless within
the caller sequentially. Now to get the used list descriptor by lookup
the whole rx descriptor by checking the in_use flag when we do descriptor
cleanup and umac reset scenario.

Signed-off-by: Karthikeyan Periyasamy <quic_periyasa@quicinc.com>
---
 drivers/net/wireless/ath/ath12k/dp.c    |  65 +++++----
 drivers/net/wireless/ath/ath12k/dp.h    |   4 +-
 drivers/net/wireless/ath/ath12k/dp_rx.c | 171 +++++++++++++++++-------
 drivers/net/wireless/ath/ath12k/dp_rx.h |   1 +
 drivers/net/wireless/ath/ath12k/ppe.c   |   8 +-
 5 files changed, 169 insertions(+), 80 deletions(-)

--- a/drivers/net/wireless/ath/ath12k/dp.c
+++ b/drivers/net/wireless/ath/ath12k/dp.c
@@ -1416,8 +1416,9 @@ int ath12k_dp_service_srng(struct ath12k
 	if (host2rxdma_mask) {
 		struct ath12k_dp *dp = &ab->dp;
 		struct dp_rxdma_ring *rx_ring = &dp->rx_refill_buf_ring;
+		LIST_HEAD(list);
 
-		ath12k_dp_rx_bufs_replenish(ab, rx_ring, 0);
+		ath12k_dp_rx_bufs_replenish(ab, rx_ring, &list, 0);
 	}
 
 	/* TODO: Implement handler for other interrupts */
@@ -1646,17 +1647,27 @@ void ath12k_dp_umac_txrx_desc_cleanup(st
 	/* RX Descriptor cleanup */
 	spin_lock_bh(&dp->rx_desc_lock);
 
-	list_for_each_entry_safe(desc_info, tmp, &dp->rx_desc_used_list, list) {
-		list_move_tail(&desc_info->list, &ab->dp.rx_desc_free_list);
-		skb = desc_info->skb;
-		desc_info->skb = NULL;
+	for (i = 0; i < ATH12K_NUM_RX_SPT_PAGES; i++) {
+		desc_info = dp->spt_info->rxbaddr[i];
+
+		for (j = 0; j < ATH12K_MAX_SPT_ENTRIES; j++) {
+			if (!desc_info[j].in_use)
+				continue;
+
+			skb = desc_info[j].skb;
+			desc_info[j].skb = NULL;
+			desc_info[j].paddr = NULL;
+			desc_info[j].in_use = false;
+			list_add_tail(&desc_info[j].list, &dp->rx_desc_free_list);
+
+			if (!skb)
+				continue;
 
-		if (!skb)
-			continue;
 
-		dma_unmap_single(ab->dev, ATH12K_SKB_RXCB(skb)->paddr,
-				 skb->len + skb_tailroom(skb), DMA_FROM_DEVICE);
-		dev_kfree_skb_any(skb);
+			dma_unmap_single(ab->dev, ATH12K_SKB_RXCB(skb)->paddr,
+					 skb->len + skb_tailroom(skb), DMA_FROM_DEVICE);
+			dev_kfree_skb_any(skb);
+		}
 	}
 
 	spin_unlock_bh(&dp->rx_desc_lock);
@@ -1692,11 +1703,11 @@ void ath12k_dp_umac_txrx_desc_cleanup(st
 
 static void ath12k_dp_cc_cleanup(struct ath12k_base *ab)
 {
-	struct ath12k_rx_desc_info *desc_info, *tmp;
+	struct ath12k_rx_desc_info *desc_info;
 	struct ath12k_tx_desc_info *tx_desc_info, *tmp1;
 	struct ath12k_dp *dp = &ab->dp;
 	struct sk_buff *skb;
-	int i;
+	int i, j;
 	u32 pool_id, tx_spt_page;
 
 	if (!dp->spt_info)
@@ -1705,16 +1716,23 @@ static void ath12k_dp_cc_cleanup(struct
 	/* RX Descriptor cleanup */
 	spin_lock_bh(&dp->rx_desc_lock);
 
-	list_for_each_entry_safe(desc_info, tmp, &dp->rx_desc_used_list, list) {
-		list_del(&desc_info->list);
-		skb = desc_info->skb;
+	for (i = 0; i < ATH12K_NUM_RX_SPT_PAGES; i++) {
+		desc_info = dp->spt_info->rxbaddr[i];
 
-		if (!skb)
-			continue;
+		for (j = 0; j < ATH12K_MAX_SPT_ENTRIES; j++) {
+			if (!desc_info[j].in_use) {
+				list_del(&desc_info[j].list);
+				continue;
+			}
+
+			skb = desc_info[j].skb;
+			if (!skb)
+				continue;
 
-		dma_unmap_single(ab->dev, ATH12K_SKB_RXCB(skb)->paddr,
-				 skb->len + skb_tailroom(skb), DMA_FROM_DEVICE);
-		dev_kfree_skb_any(skb);
+			dma_unmap_single(ab->dev, ATH12K_SKB_RXCB(skb)->paddr,
+					 skb->len + skb_tailroom(skb), DMA_FROM_DEVICE);
+			dev_kfree_skb_any(skb);
+		}
 	}
 
 	for (i = 0; i < ATH12K_NUM_RX_SPT_PAGES; i++) {
@@ -2288,7 +2306,6 @@ static int ath12k_dp_cc_init(struct ath1
 	u32 cmem_base;
 
 	INIT_LIST_HEAD(&dp->rx_desc_free_list);
-	INIT_LIST_HEAD(&dp->rx_desc_used_list);
 	spin_lock_init(&dp->rx_desc_lock);
 
 	for (i = 0; i < ATH12K_HW_MAX_QUEUES; i++) {
@@ -2512,6 +2529,7 @@ int ath12k_dp_rxdma_ring_setup(struct at
 	struct ath12k_dp *dp = &ab->dp;
 	int ret;
 	struct dp_rxdma_ring *rx_ring = &dp->rx_refill_buf_ring;
+	LIST_HEAD(list);
 
 	ret = ath12k_dp_srng_setup(ab,
 				   &dp->rx_refill_buf_ring.refill_buf_ring,
@@ -2523,7 +2541,7 @@ int ath12k_dp_rxdma_ring_setup(struct at
 		return ret;
 	}
 
-	ath12k_dp_rx_bufs_replenish(ab, rx_ring, 0);
+	ath12k_dp_rx_bufs_replenish(ab, rx_ring, &list, 0);
 	return 0;
 }
 
--- a/drivers/net/wireless/ath/ath12k/dp.h
+++ b/drivers/net/wireless/ath/ath12k/dp.h
@@ -325,7 +325,8 @@ struct ath12k_rx_desc_info {
 	struct list_head list;
 	u32 cookie;
 	dma_addr_t paddr;
-	u8 chip_id;
+	u8 chip_id		: 3,
+	   in_use		: 1;
 	struct sk_buff *skb;
 	u32 magic;
 };
@@ -448,7 +449,6 @@ struct ath12k_dp {
 	u32 num_spt_pages;
 	u32 rx_spt_base;
 	struct list_head rx_desc_free_list;
-	struct list_head rx_desc_used_list;
 	struct list_head rx_ppeds_reuse_list;
 	/* protects the free and used desc list */
 	spinlock_t rx_desc_lock;
--- a/drivers/net/wireless/ath/ath12k/dp_rx.c
+++ b/drivers/net/wireless/ath/ath12k/dp_rx.c
@@ -223,9 +223,58 @@ static int ath12k_dp_purge_mon_ring(stru
 	return -ETIMEDOUT;
 }
 
+static size_t ath12k_dp_list_cut_nodes(struct list_head *list,
+				       struct list_head *head,
+				       size_t count)
+{
+	struct list_head *cur;
+	struct ath12k_rx_desc_info *rx_desc;
+	size_t nodes = 0;
+
+	if (!count) {
+		INIT_LIST_HEAD(list);
+		goto out;
+	}
+
+	list_for_each(cur, head) {
+		if (!count)
+			break;
+
+		rx_desc = list_entry(cur, struct ath12k_rx_desc_info, list);
+		rx_desc->in_use = true;
+
+		count--;
+		nodes++;
+	}
+
+	list_cut_before(list, head, cur);
+out:
+	return nodes;
+}
+
+static void ath12k_dp_rx_enqueue_free(struct ath12k_dp *dp,
+				      struct list_head *used_list)
+{
+	struct list_head *cur;
+	struct ath12k_rx_desc_info *rx_desc;
+
+	spin_lock_bh(&dp->rx_desc_lock);
+
+	/* Reset the use flag */
+	list_for_each(cur, used_list) {
+		rx_desc = list_entry(cur, struct ath12k_rx_desc_info, list);
+		rx_desc->in_use = false;
+	}
+
+	list_splice_tail(used_list, &dp->rx_desc_free_list);
+
+	spin_unlock_bh(&dp->rx_desc_lock);
+}
+
 /* Returns number of Rx buffers replenished */
 int ath12k_dp_rx_bufs_replenish(struct ath12k_base *ab,
 				struct dp_rxdma_ring *rx_ring,
+				struct list_head *used_list,
 				int req_entries)
 {
 	struct ath12k_buffer_addr *desc;
@@ -254,6 +303,19 @@ int ath12k_dp_rx_bufs_replenish(struct a
 	req_entries = min(num_free, req_entries);
 	num_remain = req_entries;
 
+	if (!num_remain)
+		goto skip_replenish;
+
+	/* Get the descriptor from free list */
+	if (list_empty(used_list)) {
+		spin_lock_bh(&dp->rx_desc_lock);
+		req_entries = ath12k_dp_list_cut_nodes(used_list,
+						       &dp->rx_desc_free_list,
+						       num_remain);
+		spin_unlock_bh(&dp->rx_desc_lock);
+		num_remain = req_entries;
+	}
+
 	while (num_remain > 0) {
 		skb = dev_alloc_skb(DP_RX_BUFFER_SIZE +
 				    DP_RX_BUFFER_ALIGN_SIZE);
@@ -273,34 +335,21 @@ int ath12k_dp_rx_bufs_replenish(struct a
 		if (dma_mapping_error(ab->dev, paddr))
 			goto fail_free_skb;
 
-		spin_lock_bh(&dp->rx_desc_lock);
-
-		/* Get desc from free list and store in used list
-		 * for cleanup purposes
-		 *
-		 * TODO: pass the removed descs rather than
-		 * add/read to optimize
-		 */
-		rx_desc = list_first_entry_or_null(&dp->rx_desc_free_list,
+		rx_desc = list_first_entry_or_null(used_list,
 						   struct ath12k_rx_desc_info,
 						   list);
-		if (!rx_desc) {
-			spin_unlock_bh(&dp->rx_desc_lock);
+		if (!rx_desc)
 			goto fail_dma_unmap;
-		}
 
 		rx_desc->skb = skb;
 		rx_desc->paddr = paddr;
 		cookie = rx_desc->cookie;
-		list_del(&rx_desc->list);
-		list_add_tail(&rx_desc->list, &dp->rx_desc_used_list);
-
-		spin_unlock_bh(&dp->rx_desc_lock);
 
 		desc = ath12k_hal_srng_src_get_next_entry(ab, srng);
 		if (!desc)
-			goto fail_buf_unassign;
+			goto fail_dma_unmap;
 
+		list_del(&rx_desc->list);
 		ATH12K_SKB_RXCB(skb)->paddr = paddr;
 
 		num_remain--;
@@ -308,18 +357,16 @@ int ath12k_dp_rx_bufs_replenish(struct a
 		ath12k_hal_rx_buf_addr_info_set(desc, paddr, cookie, mgr);
 	}
 
+skip_replenish:
 	ath12k_hal_srng_access_end(ab, srng);
 
+	if (!list_empty(used_list))
+		ath12k_dp_rx_enqueue_free(dp, used_list);
+
 	spin_unlock_bh(&srng->lock);
 
 	return req_entries - num_remain;
 
-fail_buf_unassign:
-	spin_lock_bh(&dp->rx_desc_lock);
-	list_del(&rx_desc->list);
-	list_add_tail(&rx_desc->list, &dp->rx_desc_free_list);
-	rx_desc->skb = NULL;
-	spin_unlock_bh(&dp->rx_desc_lock);
 fail_dma_unmap:
 	dma_unmap_single_attrs(ab->dev, paddr, skb->len + skb_tailroom(skb),
 			       DMA_FROM_DEVICE, DMA_ATTR_SKIP_CPU_SYNC);
@@ -328,6 +375,9 @@ fail_free_skb:
 
 	ath12k_hal_srng_access_end(ab, srng);
 
+	if (!list_empty(used_list))
+		ath12k_dp_rx_enqueue_free(dp, used_list);
+
 	spin_unlock_bh(&srng->lock);
 
 	return req_entries - num_remain;
@@ -378,6 +428,7 @@ static int ath12k_dp_rxdma_ring_buf_setu
 					  struct dp_rxdma_ring *rx_ring,
 					  u32 ringtype)
 {
+	LIST_HEAD(list);
 	int num_entries;
 
 	num_entries = rx_ring->refill_buf_ring.size /
@@ -387,7 +438,7 @@ static int ath12k_dp_rxdma_ring_buf_setu
 	if ((ringtype == HAL_RXDMA_MONITOR_BUF) || (ringtype == HAL_TX_MONITOR_BUF))
 		ath12k_dp_mon_buf_replenish(ab, rx_ring, num_entries);
 	else
-		ath12k_dp_rx_bufs_replenish(ab, rx_ring, 0);
+		ath12k_dp_rx_bufs_replenish(ab, rx_ring, &list, 0);
 	return 0;
 }
 
@@ -3498,11 +3549,15 @@ int ath12k_dp_rx_process(struct ath12k_b
 	struct ath12k_link_sta *arsta = NULL;
 	struct ath12k_peer *peer = NULL;
 	struct ath12k *ar;
-	u8 hw_link_id;
+	u8 hw_link_id, chip_id;
 	int valid_entries;
+	struct list_head rx_desc_used_list[ATH12K_MAX_SOCS];
 
 	__skb_queue_head_init(&msdu_list);
 
+	for (i = 0; i < ATH12K_MAX_SOCS; i++)
+		INIT_LIST_HEAD(&rx_desc_used_list[i]);
+
 	srng = &ab->hal.srng_list[dp->reo_dst_ring[ring_id].ring_id];
 
 	spin_lock_bh(&srng->lock);
@@ -3567,12 +3622,13 @@ try_again:
 		if (unlikely(desc_info->magic != ATH12K_DP_RX_DESC_MAGIC))
 			ath12k_warn(ab, "Check HW CC implementation");
 
+		chip_id = src_ab->chip_id;
+
 		msdu = desc_info->skb;
 		desc_info->skb = NULL;
+		desc_info->paddr = NULL;
 
-		spin_lock_bh(&src_ab->dp.rx_desc_lock);
-		list_move_tail(&desc_info->list, &src_ab->dp.rx_desc_free_list);
-		spin_unlock_bh(&src_ab->dp.rx_desc_lock);
+		list_add_tail(&desc_info->list, &rx_desc_used_list[chip_id]);
 
 		rxcb = ATH12K_SKB_RXCB(msdu);
 		dmac_inv_range_no_dsb(msdu->data, msdu->data + (msdu->len + skb_tailroom(msdu)));
@@ -3580,8 +3636,8 @@ try_again:
 				       msdu->len + skb_tailroom(msdu),
 				       DMA_FROM_DEVICE, DMA_ATTR_SKIP_CPU_SYNC);
 
-		num_buffs_reaped[src_ab->chip_id]++;
-		ab->soc_stats.reo_rx[ring_id][src_ab->chip_id]++;
+		num_buffs_reaped[chip_id]++;
+		ab->soc_stats.reo_rx[ring_id][chip_id]++;
 
 		push_reason = le32_get_bits(desc.info0,
 					    HAL_REO_DEST_RING_INFO0_PUSH_REASON);
@@ -3659,7 +3715,9 @@ try_again:
 
 		rx_ring = &src_ab->dp.rx_refill_buf_ring;
 
-		ath12k_dp_rx_bufs_replenish(src_ab, rx_ring, num_buffs_reaped[i]);
+		ath12k_dp_rx_bufs_replenish(src_ab, rx_ring,
+					    &rx_desc_used_list[i],
+					    num_buffs_reaped[i]);
 	}
 
 	ath12k_dp_rx_process_received_packets(ab, napi, &msdu_list,
@@ -3979,9 +4037,9 @@ static int ath12k_dp_rx_h_defrag_reo_rei
 	}
 
 	desc_info->skb = defrag_skb;
+	desc_info->in_use = true;
 
 	list_del(&desc_info->list);
-	list_add_tail(&desc_info->list, &dp->rx_desc_used_list);
 	spin_unlock_bh(&dp->rx_desc_lock);
 
 	ATH12K_SKB_RXCB(defrag_skb)->paddr = buf_paddr;
@@ -4043,9 +4101,9 @@ static int ath12k_dp_rx_h_defrag_reo_rei
 
 err_free_desc:
 	spin_lock_bh(&dp->rx_desc_lock);
-	list_del(&desc_info->list);
-	list_add_tail(&desc_info->list, &dp->rx_desc_free_list);
 	desc_info->skb = NULL;
+	desc_info->in_use = false;
+	list_add_tail(&desc_info->list, &dp->rx_desc_free_list);
 	spin_unlock_bh(&dp->rx_desc_lock);
 err_unmap_dma:
 	dma_unmap_single(ab->dev, buf_paddr, defrag_skb->len + skb_tailroom(defrag_skb),
@@ -4262,7 +4320,8 @@ out_unlock:
 
 static int
 ath12k_dp_process_rx_err_buf(struct ath12k *ar, struct hal_reo_dest_ring *desc,
-			     bool drop, u32 cookie)
+			     bool drop, u32 cookie,
+			     struct list_head *list)
 {
 	struct ath12k_base *ab = ar->ab;
 	struct sk_buff *msdu;
@@ -4291,9 +4350,7 @@ ath12k_dp_process_rx_err_buf(struct ath1
 
 	msdu = desc_info->skb;
 	desc_info->skb = NULL;
-	spin_lock_bh(&ab->dp.rx_desc_lock);
-	list_move_tail(&desc_info->list, &ab->dp.rx_desc_free_list);
-	spin_unlock_bh(&ab->dp.rx_desc_lock);
+	list_add_tail(&desc_info->list, list);
 
 	rxcb = ATH12K_SKB_RXCB(msdu);
 	dma_unmap_single(ar->ab->dev, rxcb->paddr,
@@ -4339,7 +4396,8 @@ exit:
 }
 
 static int ath12k_dp_h_msdu_buffer_type(struct ath12k_base *ab,
-					struct hal_reo_dest_ring *desc)
+					struct hal_reo_dest_ring *desc,
+					struct list_head *list)
 {
 	struct ath12k_rx_desc_info *desc_info;
 	struct sk_buff *msdu;
@@ -4362,9 +4420,7 @@ static int ath12k_dp_h_msdu_buffer_type(
 
 	msdu = desc_info->skb;
 	desc_info->skb = NULL;
-	spin_lock_bh(&ab->dp.rx_desc_lock);
-	list_move_tail(&desc_info->list, &ab->dp.rx_desc_free_list);
-	spin_unlock_bh(&ab->dp.rx_desc_lock);
+	list_add_tail(&desc_info->list, list);
 	rxcb = ATH12K_SKB_RXCB(msdu);
 	dma_unmap_single(ab->dev, rxcb->paddr, msdu->len + skb_tailroom(msdu),
 			 DMA_FROM_DEVICE);
@@ -4395,6 +4451,7 @@ int ath12k_dp_rx_process_err(struct ath1
 	char buf[64] = {0};
 	u8 hw_link_id, chip_id;
 	int num_buffs_reaped[ATH12K_MAX_SOCS] = { };
+	struct list_head rx_desc_used_list[ATH12K_MAX_SOCS];
 
 	tot_n_bufs_reaped = 0;
 	quota = budget;
@@ -4402,6 +4459,9 @@ int ath12k_dp_rx_process_err(struct ath1
 	dp = &ab->dp;
 	reo_except = &dp->reo_except_ring;
 
+	for (i = 0; i < ATH12K_MAX_SOCS; i++)
+		INIT_LIST_HEAD(&rx_desc_used_list[i]);
+
 	srng = &ab->hal.srng_list[reo_except->ring_id];
 
 	spin_lock_bh(&srng->lock);
@@ -4448,7 +4508,8 @@ int ath12k_dp_rx_process_err(struct ath1
 		 */
 		if (u32_get_bits(reo_desc->info0, HAL_REO_DEST_RING_INFO0_BUFFER_TYPE) ==
 			HAL_REO_DEST_RING_BUFFER_TYPE_MSDU) {
-			if (!ath12k_dp_h_msdu_buffer_type(src_ab, reo_desc)) {
+			if (!ath12k_dp_h_msdu_buffer_type(src_ab, reo_desc,
+							  &rx_desc_used_list[chip_id])) {
 				num_buffs_reaped[chip_id]++;
 				tot_n_bufs_reaped++;
 			}
@@ -4495,7 +4556,8 @@ int ath12k_dp_rx_process_err(struct ath1
 		for (i = 0; i < num_msdus; i++) {
 
 			if (!ath12k_dp_process_rx_err_buf(ar, reo_desc, drop,
-							  msdu_cookies[i])) {
+							  msdu_cookies[i],
+							  &rx_desc_used_list[chip_id])) {
 				num_buffs_reaped[chip_id]++;
 				tot_n_bufs_reaped++;
 			}
@@ -4525,7 +4587,9 @@ exit:
 
 		rx_ring = &src_ab->dp.rx_refill_buf_ring;
 
-		ath12k_dp_rx_bufs_replenish(src_ab, rx_ring, num_buffs_reaped[i]);
+		ath12k_dp_rx_bufs_replenish(src_ab, rx_ring,
+					    &rx_desc_used_list[i],
+					    num_buffs_reaped[i]);
 	}
 
 	return tot_n_bufs_reaped;
@@ -4884,12 +4948,16 @@ int ath12k_dp_rx_process_wbm_err(struct
 	struct ath12k_soc_dp_stats *soc_stats = &ab->soc_stats;
 	int total_num_buffs_reaped = 0;
 	int ret;
-	u8 hw_link_id;
+	u8 hw_link_id, chip_id;
 	char buf[64] = {0};
+	struct list_head rx_desc_used_list[ATH12K_MAX_SOCS];
 
 	__skb_queue_head_init(&msdu_list);
 	__skb_queue_head_init(&scatter_msdu_list);
 
+	for (i = 0; i < ATH12K_MAX_SOCS; i++)
+		INIT_LIST_HEAD(&rx_desc_used_list[i]);
+
 	srng = &ab->hal.srng_list[dp->rx_rel_ring.ring_id];
 
 	spin_lock_bh(&srng->lock);
@@ -4927,17 +4995,16 @@ int ath12k_dp_rx_process_wbm_err(struct
 		desc_info->skb = NULL;
 		src_ab = ab->ag->ab[desc_info->chip_id];
 		dp = &src_ab->dp;
+		chip_id = src_ab->chip_id;
 
-		spin_lock_bh(&dp->rx_desc_lock);
-		list_move_tail(&desc_info->list, &dp->rx_desc_free_list);
-		spin_unlock_bh(&dp->rx_desc_lock);
+		list_add_tail(&desc_info->list, &rx_desc_used_list[chip_id]);
 
 		rxcb = ATH12K_SKB_RXCB(msdu);
 		dma_unmap_single(src_ab->dev, rxcb->paddr,
 				 msdu->len + skb_tailroom(msdu),
 				 DMA_FROM_DEVICE);
 
-		num_buffs_reaped[src_ab->chip_id]++;
+		num_buffs_reaped[chip_id]++;
 		total_num_buffs_reaped++;
 
 		if (!err_info.continuation)
@@ -5009,7 +5076,9 @@ int ath12k_dp_rx_process_wbm_err(struct
 
 		rx_ring = &src_ab->dp.rx_refill_buf_ring;
 
-		ath12k_dp_rx_bufs_replenish(src_ab, rx_ring, num_buffs_reaped[i]);
+		ath12k_dp_rx_bufs_replenish(src_ab, rx_ring,
+					    &rx_desc_used_list[i],
+					    num_buffs_reaped[i]);
 	}
 
 	rcu_read_lock();
--- a/drivers/net/wireless/ath/ath12k/dp_rx.h
+++ b/drivers/net/wireless/ath/ath12k/dp_rx.h
@@ -203,6 +203,7 @@ int ath12k_dp_rx_process(struct ath12k_b
 			 int budget);
 int ath12k_dp_rx_bufs_replenish(struct ath12k_base *ab,
 				struct dp_rxdma_ring *rx_ring,
+				struct list_head *used_list,
 				int req_entries);
 int ath12k_dp_rx_pdev_mon_attach(struct ath12k *ar);
 void ath12k_dp_rx_pdev_mon_detach(struct ath12k_base *ab, const int pdev_idx);
--- a/drivers/net/wireless/ath/ath12k/ppe.c
+++ b/drivers/net/wireless/ath/ath12k/ppe.c
@@ -141,10 +141,14 @@ static bool ath12k_ppeds_free_rx_desc(st
         	return false;
 
 	skb = rx_desc->skb;
-	rx_desc->skb= NULL;
+
 	spin_lock_bh(&ab->dp.rx_desc_lock);
-	list_move_tail(&rx_desc->list, &ab->dp.rx_desc_free_list);
+	rx_desc->skb = NULL;
+	rx_desc->paddr = NULL;
+	rx_desc->in_use = false;
+	list_add_tail(&rx_desc->list, &ab->dp.rx_desc_free_list);
 	spin_unlock_bh(&ab->dp.rx_desc_lock);
+
 	if (!skb) {
 		ath12k_err(ab, "ppeds rx desc with no skb when freeing\n");
 		return false;
