From 3a22b800812ab6b79b14114c3d14f4cc5293f7d4 Mon Sep 17 00:00:00 2001
From: Tian Yang <tiany@codeaurora.org>
Date: Tue, 29 Dec 2015 13:02:21 -0600
Subject: [PATCH 359/500] skbuff_debug: move activate/deactivate sites

This way we can see if next/prev in SKB was changed
while the SKB was not being used.

Change-Id: I281267e230d3406181a07d095a76ae6bc58e2c8d
Signed-off-by: Matthew McClintock <mmcclint@codeaurora.org>
Signed-off-by: Casey Chen <kexinc@codeaurora.org>
Signed-off-by: Tian Yang <tiany@codeaurora.org>
---
 net/core/skbuff.c | 10 ++++++----
 1 file changed, 6 insertions(+), 4 deletions(-)

diff --git a/net/core/skbuff.c b/net/core/skbuff.c
index 545a7fc28580..186e4e504741 100644
--- a/net/core/skbuff.c
+++ b/net/core/skbuff.c
@@ -394,7 +394,6 @@ static inline void __finalize_skb_around(struct sk_buff *skb, void *data,
 	memset(shinfo, 0, offsetof(struct skb_shared_info, dataref));
 	atomic_set(&shinfo->dataref, 1);
 	skb_set_kcov_handle(skb, kcov_common_handle());
-	skbuff_debugobj_init_and_activate(skb);
 }
 
 static inline void *__slab_build_skb(struct sk_buff *skb, void *data,
@@ -479,6 +478,7 @@ struct sk_buff *__build_skb(void *data, unsigned int frag_size)
 	skb = kmem_cache_alloc(skbuff_cache, GFP_ATOMIC);
 	if (unlikely(!skb))
 		return NULL;
+	skbuff_debugobj_init_and_activate(skb);
 
 	memset(skb, 0, offsetof(struct sk_buff, tail));
 	__build_skb_around(skb, data, frag_size);
@@ -675,6 +675,7 @@ struct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask,
 		skb = kmem_cache_alloc_node(cache, gfp_mask & ~GFP_DMA, node);
 	if (unlikely(!skb))
 		return NULL;
+	skbuff_debugobj_init_and_activate(skb);
 	prefetchw(skb);
 
 	/* We do our best to align skb_shared_info on a separate cache
@@ -709,10 +710,10 @@ struct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask,
 		refcount_set(&fclones->fclone_ref, 1);
 	}
 
-	skbuff_debugobj_init_and_activate(skb);
 	return skb;
 
 nodata:
+	skbuff_debugobj_deactivate(skb);
 	kmem_cache_free(cache, skb);
 	return NULL;
 }
@@ -2130,9 +2131,9 @@ struct sk_buff *skb_clone(struct sk_buff *skb, gfp_t gfp_mask)
 		n = kmem_cache_alloc(skbuff_cache, gfp_mask);
 		if (!n)
 			return NULL;
+		skbuff_debugobj_init_and_activate(n);
 
 		n->fclone = SKB_FCLONE_UNAVAILABLE;
-		skbuff_debugobj_init_and_activate(n);
 	}
 
 	return __skb_clone(n, skb);
@@ -5948,8 +5949,9 @@ void kfree_skb_partial(struct sk_buff *skb, bool head_stolen)
 {
 	if (head_stolen) {
 		skb_release_head_state(skb);
-		kmem_cache_free(skbuff_cache, skb);
+
 		skbuff_debugobj_deactivate(skb);
+		kmem_cache_free(skbuff_cache, skb);
 	} else {
 		__kfree_skb(skb);
 	}
-- 
2.34.1

