From 371e84585704ddcd5771ab817d71f7e7a146f436 Mon Sep 17 00:00:00 2001
From: Md Sadre Alam <quic_mdalam@quicinc.com>
Date: Fri, 3 Nov 2023 12:55:53 +0530
Subject: [PATCH 478/500] crypto: qce - Add cmd descriptor support for aead

Add cmd descriptor support for aead algorithm.
with cmd descriptor support all regaiter read/write
will goi via bam.

Change-Id: Ib32d8a18f3cd8a00c7e1767dbf67aefd06ccf6a7
Signed-off-by: Md Sadre Alam <quic_mdalam@quicinc.com>
---
 drivers/crypto/qce/aead.c     |  19 ++++-
 drivers/crypto/qce/common.c   | 151 ++++++++++++++++++++++++++++++++--
 drivers/crypto/qce/common.h   |   2 +-
 drivers/crypto/qce/sha.c      |   5 +-
 drivers/crypto/qce/skcipher.c |   2 +-
 5 files changed, 166 insertions(+), 13 deletions(-)

diff --git a/drivers/crypto/qce/aead.c b/drivers/crypto/qce/aead.c
index 7d811728f047..bad6ebe22bda 100644
--- a/drivers/crypto/qce/aead.c
+++ b/drivers/crypto/qce/aead.c
@@ -29,6 +29,7 @@ static void qce_aead_done(void *data)
 	struct qce_alg_template *tmpl = to_aead_tmpl(crypto_aead_reqtfm(req));
 	struct qce_device *qce = tmpl->qce;
 	struct qce_result_dump *result_buf = qce->dma.result_buf;
+	struct qce_bam_transaction *qce_bam_txn = qce->dma.qce_bam_txn;
 	enum dma_data_direction dir_src, dir_dst;
 	bool diff_dst;
 	int error;
@@ -50,6 +51,19 @@ static void qce_aead_done(void *data)
 
 	dma_unmap_sg(qce->dev, rctx->dst_sg, rctx->dst_nents, dir_dst);
 
+	if (qce->qce_cmd_desc_enable) {
+		if (qce_bam_txn->qce_read_sgl_cnt)
+			dma_unmap_sg(qce->dev,
+				qce_bam_txn->qce_reg_read_sgl,
+				qce_bam_txn->qce_read_sgl_cnt,
+				DMA_DEV_TO_MEM);
+		if (qce_bam_txn->qce_write_sgl_cnt)
+			dma_unmap_sg(qce->dev,
+				qce_bam_txn->qce_reg_write_sgl,
+				qce_bam_txn->qce_write_sgl_cnt,
+				DMA_MEM_TO_DEV);
+	}
+
 	if (IS_CCM(rctx->flags)) {
 		if (req->assoclen) {
 			sg_free_table(&rctx->src_tbl);
@@ -437,6 +451,9 @@ qce_aead_async_req_handle(struct crypto_async_request *async_req)
 	dir_src = diff_dst ? DMA_TO_DEVICE : DMA_BIDIRECTIONAL;
 	dir_dst = diff_dst ? DMA_FROM_DEVICE : DMA_BIDIRECTIONAL;
 
+	if (qce->qce_cmd_desc_enable)
+		qce_read_dma_get_lock(qce);
+
 	if (IS_CCM(rctx->flags)) {
 		ret = qce_aead_create_ccm_nonce(rctx, ctx);
 		if (ret)
@@ -475,7 +492,7 @@ qce_aead_async_req_handle(struct crypto_async_request *async_req)
 
 	qce_dma_issue_pending(&qce->dma);
 
-	ret = qce_start(async_req, tmpl->crypto_alg_type);
+	ret = qce_start(async_req, tmpl->crypto_alg_type, qce);
 	if (ret)
 		goto error_terminate;
 
diff --git a/drivers/crypto/qce/common.c b/drivers/crypto/qce/common.c
index 4d2788c79845..8c02aa1064cf 100644
--- a/drivers/crypto/qce/common.c
+++ b/drivers/crypto/qce/common.c
@@ -120,6 +120,14 @@ static void qce_setup_config(struct qce_device *qce)
 	qce_write(qce, REG_CONFIG, config);
 }
 
+static inline void qce_crypto_go_dma(struct qce_device *qce, bool result_dump)
+{
+	if (result_dump)
+		qce_write_reg_dma(qce, REG_GOPROC, BIT(GO_SHIFT) | BIT(RESULTS_DUMP_SHIFT), 1);
+	else
+		qce_write_reg_dma(qce, REG_GOPROC, BIT(GO_SHIFT), 1);
+}
+
 static inline void qce_crypto_go(struct qce_device *qce, bool result_dump)
 {
 	if (result_dump)
@@ -196,7 +204,6 @@ static int qce_setup_regs_ahash_dma(struct crypto_async_request *async_req)
 	qce_clear_bam_transaction(qce);
 
 	qce_setup_config_dma(qce);
-
 	if (IS_CMAC(rctx->flags)) {
 		qce_write_reg_dma(qce, REG_AUTH_SEG_CFG, 0, 1);
 		qce_write_reg_dma(qce, REG_ENCR_SEG_CFG, 0, 1);
@@ -682,6 +689,137 @@ static unsigned int qce_be32_to_cpu_array(u32 *dst, const u8 *src, unsigned int
 	return DIV_ROUND_UP(len, sizeof(u32));
 }
 
+static int qce_setup_regs_aead_dma(struct crypto_async_request *async_req)
+{
+	struct aead_request *req = aead_request_cast(async_req);
+	struct qce_aead_reqctx *rctx = aead_request_ctx(req);
+	struct qce_aead_ctx *ctx = crypto_tfm_ctx(async_req->tfm);
+	struct qce_alg_template *tmpl = to_aead_tmpl(crypto_aead_reqtfm(req));
+	struct qce_device *qce = tmpl->qce;
+	u32 enckey[QCE_MAX_CIPHER_KEY_SIZE / sizeof(u32)] = {0};
+	u32 enciv[QCE_MAX_IV_SIZE / sizeof(u32)] = {0};
+	u32 authkey[QCE_SHA_HMAC_KEY_SIZE / sizeof(u32)] = {0};
+	u32 authiv[SHA256_DIGEST_SIZE / sizeof(u32)] = {0};
+	u32 authnonce[QCE_MAX_NONCE / sizeof(u32)] = {0};
+	unsigned int enc_keylen = ctx->enc_keylen;
+	unsigned int auth_keylen = ctx->auth_keylen;
+	unsigned int enc_ivsize = rctx->ivsize;
+	unsigned int auth_ivsize = 0;
+	unsigned int enckey_words, enciv_words;
+	unsigned int authkey_words, authiv_words, authnonce_words;
+	unsigned long flags = rctx->flags;
+	u32 encr_cfg, auth_cfg, config, totallen;
+	u32 iv_last_word;
+	int ret;
+
+	qce_clear_bam_transaction(qce);
+	qce_setup_config_dma(qce);
+
+	/* Write encryption key */
+	enckey_words = qce_be32_to_cpu_array(enckey, ctx->enc_key, enc_keylen);
+	qce_write_array_dma(qce, REG_ENCR_KEY0, enckey, enckey_words);
+
+	/* Write encryption iv */
+	enciv_words = qce_be32_to_cpu_array(enciv, rctx->iv, enc_ivsize);
+	qce_write_array_dma(qce, REG_CNTR0_IV0, enciv, enciv_words);
+
+	if (IS_CCM(rctx->flags)) {
+		iv_last_word = enciv[enciv_words - 1];
+		qce_write_reg_dma(qce, REG_CNTR3_IV3, iv_last_word + 1, 1);
+		qce_write_array_dma(qce, REG_ENCR_CCM_INT_CNTR0, (u32 *)enciv, enciv_words);
+		qce_write_reg_dma(qce, REG_CNTR_MASK, ~0, 1);
+		qce_write_reg_dma(qce, REG_CNTR_MASK0, ~0, 1);
+		qce_write_reg_dma(qce, REG_CNTR_MASK1, ~0, 1);
+		qce_write_reg_dma(qce, REG_CNTR_MASK2, ~0, 1);
+	}
+
+	/* Clear authentication IV and KEY registers of previous values */
+	qce_clear_array_dma(qce, REG_AUTH_IV0, 16);
+	qce_clear_array_dma(qce, REG_AUTH_KEY0, 16);
+
+	/* Clear byte count */
+	qce_clear_array_dma(qce, REG_AUTH_BYTECNT0, 4);
+
+	/* Write authentication key */
+	authkey_words = qce_be32_to_cpu_array(authkey, ctx->auth_key, auth_keylen);
+	qce_write_array_dma(qce, REG_AUTH_KEY0, (u32 *)authkey, authkey_words);
+
+	/* Write initial authentication IV only for HMAC algorithms */
+	if (IS_SHA_HMAC(rctx->flags)) {
+		/* Write default authentication iv */
+		if (IS_SHA1_HMAC(rctx->flags)) {
+			auth_ivsize = SHA1_DIGEST_SIZE;
+			memcpy(authiv, std_iv_sha1, auth_ivsize);
+		} else if (IS_SHA256_HMAC(rctx->flags)) {
+			auth_ivsize = SHA256_DIGEST_SIZE;
+			memcpy(authiv, std_iv_sha256, auth_ivsize);
+		}
+		authiv_words = auth_ivsize / sizeof(u32);
+		qce_write_array_dma(qce, REG_AUTH_IV0, (u32 *)authiv, authiv_words);
+	} else if (IS_CCM(rctx->flags)) {
+		/* Write nonce for CCM algorithms */
+		authnonce_words = qce_be32_to_cpu_array(authnonce, rctx->ccm_nonce, QCE_MAX_NONCE);
+		qce_write_array_dma(qce, REG_AUTH_INFO_NONCE0, authnonce, authnonce_words);
+	}
+
+	/* Set up ENCR_SEG_CFG */
+	encr_cfg = qce_encr_cfg(flags, enc_keylen);
+	if (IS_ENCRYPT(flags))
+		encr_cfg |= BIT(ENCODE_SHIFT);
+	qce_write_reg_dma(qce, REG_ENCR_SEG_CFG, encr_cfg, 1);
+
+	/* Set up AUTH_SEG_CFG */
+	auth_cfg = qce_auth_cfg(rctx->flags, auth_keylen, ctx->authsize);
+	auth_cfg |= BIT(AUTH_LAST_SHIFT);
+	auth_cfg |= BIT(AUTH_FIRST_SHIFT);
+	if (IS_ENCRYPT(flags)) {
+		if (IS_CCM(rctx->flags))
+			auth_cfg |= AUTH_POS_BEFORE << AUTH_POS_SHIFT;
+		else
+			auth_cfg |= AUTH_POS_AFTER << AUTH_POS_SHIFT;
+	} else {
+		if (IS_CCM(rctx->flags))
+			auth_cfg |= AUTH_POS_AFTER << AUTH_POS_SHIFT;
+		else
+			auth_cfg |= AUTH_POS_BEFORE << AUTH_POS_SHIFT;
+	}
+	qce_write_reg_dma(qce, REG_AUTH_SEG_CFG, auth_cfg, 1);
+
+	totallen = rctx->cryptlen + rctx->assoclen;
+
+	/* Set the encryption size and start offset */
+	if (IS_CCM(rctx->flags) && IS_DECRYPT(rctx->flags))
+		qce_write_reg_dma(qce, REG_ENCR_SEG_SIZE, rctx->cryptlen + ctx->authsize, 1);
+	else
+		qce_write_reg_dma(qce, REG_ENCR_SEG_SIZE, rctx->cryptlen, 1);
+	qce_write_reg_dma(qce, REG_ENCR_SEG_START, rctx->assoclen & 0xffff, 1);
+
+	/* Set the authentication size and start offset */
+	qce_write_reg_dma(qce, REG_AUTH_SEG_SIZE, totallen, 1);
+	qce_write_reg_dma(qce, REG_AUTH_SEG_START, 0, 1);
+
+	/* Write total length */
+	if (IS_CCM(rctx->flags) && IS_DECRYPT(rctx->flags))
+		qce_write_reg_dma(qce, REG_SEG_SIZE, totallen + ctx->authsize, 1);
+	else
+		qce_write_reg_dma(qce, REG_SEG_SIZE, totallen, 1);
+
+	/* get little endianness */
+	config = qce_config_reg(qce, 1);
+	qce_write_reg_dma(qce, REG_CONFIG, config, 1);
+
+	/* Start the process */
+	qce_crypto_go_dma(qce, !IS_CCM(flags));
+
+	ret = qce_submit_cmd_desc(qce, 0);
+	if (ret) {
+		dev_err(qce->dev, "Error in submitting cmd descriptor\n");
+		return ret;
+	}
+
+	return 0;
+}
+
 static int qce_setup_regs_aead(struct crypto_async_request *async_req)
 {
 	struct aead_request *req = aead_request_cast(async_req);
@@ -806,11 +944,9 @@ static int qce_setup_regs_aead(struct crypto_async_request *async_req)
 }
 #endif
 
-int qce_start(struct crypto_async_request *async_req, u32 type)
+int qce_start(struct crypto_async_request *async_req, u32 type,
+		struct qce_device *qce)
 {
-	struct skcipher_request *req = skcipher_request_cast(async_req);
-	struct qce_alg_template *tmpl = to_cipher_tmpl(crypto_skcipher_reqtfm(req));
-	struct qce_device *qce = tmpl->qce;
 	switch (type) {
 #ifdef CONFIG_CRYPTO_DEV_QCE_SKCIPHER
 	case CRYPTO_ALG_TYPE_SKCIPHER:
@@ -828,7 +964,10 @@ int qce_start(struct crypto_async_request *async_req, u32 type)
 #endif
 #ifdef CONFIG_CRYPTO_DEV_QCE_AEAD
 	case CRYPTO_ALG_TYPE_AEAD:
-		return qce_setup_regs_aead(async_req);
+		if (qce->qce_cmd_desc_enable)
+			return qce_setup_regs_aead_dma(async_req);
+		else
+			return qce_setup_regs_aead(async_req);
 #endif
 	default:
 		return -EINVAL;
diff --git a/drivers/crypto/qce/common.h b/drivers/crypto/qce/common.h
index 02e63ad9f245..e244aa940d1e 100644
--- a/drivers/crypto/qce/common.h
+++ b/drivers/crypto/qce/common.h
@@ -99,6 +99,6 @@ struct qce_alg_template {
 void qce_cpu_to_be32p_array(__be32 *dst, const u8 *src, unsigned int len);
 int qce_check_status(struct qce_device *qce, u32 *status);
 void qce_get_version(struct qce_device *qce, u32 *major, u32 *minor, u32 *step);
-int qce_start(struct crypto_async_request *async_req, u32 type);
+int qce_start(struct crypto_async_request *async_req, u32 type, struct qce_device *qce);
 
 #endif /* _COMMON_H_ */
diff --git a/drivers/crypto/qce/sha.c b/drivers/crypto/qce/sha.c
index e4a77d495478..8b4a5d7968ae 100644
--- a/drivers/crypto/qce/sha.c
+++ b/drivers/crypto/qce/sha.c
@@ -95,7 +95,6 @@ static int qce_ahash_async_req_handle(struct crypto_async_request *async_req)
 	struct qce_device *qce = tmpl->qce;
 	unsigned long flags = rctx->flags;
 	int ret;
-
 	if (IS_SHA_HMAC(flags)) {
 		rctx->authkey = ctx->authkey;
 		rctx->authklen = QCE_SHA_HMAC_KEY_SIZE;
@@ -107,7 +106,6 @@ static int qce_ahash_async_req_handle(struct crypto_async_request *async_req)
 	/* Get the LOCK for this request */
 	if (qce->qce_cmd_desc_enable)
 		qce_read_dma_get_lock(qce);
-
 	rctx->src_nents = sg_nents_for_len(req->src, req->nbytes);
 	if (rctx->src_nents < 0) {
 		dev_err(qce->dev, "Invalid numbers of src SG.\n");
@@ -132,8 +130,7 @@ static int qce_ahash_async_req_handle(struct crypto_async_request *async_req)
 		goto error_unmap_dst;
 
 	qce_dma_issue_pending(&qce->dma);
-
-	ret = qce_start(async_req, tmpl->crypto_alg_type);
+	ret = qce_start(async_req, tmpl->crypto_alg_type, qce);
 	if (ret)
 		goto error_terminate;
 
diff --git a/drivers/crypto/qce/skcipher.c b/drivers/crypto/qce/skcipher.c
index 02adc8b5b2ec..2201d3d5883c 100644
--- a/drivers/crypto/qce/skcipher.c
+++ b/drivers/crypto/qce/skcipher.c
@@ -175,7 +175,7 @@ qce_skcipher_async_req_handle(struct crypto_async_request *async_req)
 
 	qce_dma_issue_pending(&qce->dma);
 
-	ret = qce_start(async_req, tmpl->crypto_alg_type);
+	ret = qce_start(async_req, tmpl->crypto_alg_type, qce);
 	if (ret)
 		goto error_terminate;
 
-- 
2.34.1

